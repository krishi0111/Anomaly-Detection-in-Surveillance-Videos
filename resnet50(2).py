# -*- coding: utf-8 -*-
"""Resnet50(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9scN-HNkn62TSSTzqI0gUhAIaccrs51
"""

# Step 1: Install Kaggle package (if not installed)
!pip install kaggle --quiet

# Step 2: Create the Kaggle directory
!mkdir -p ~/.kaggle

# Step 3: Create kaggle.json file directly in Colab (Replace with your Kaggle API credentials)
kaggle_api_key = """{
    "username":"krishithakkar",
    "key":"cad4c20c4da936e960854bb8da5e1289"
}"""

with open("/root/.kaggle/kaggle.json", "w") as f:
    f.write(kaggle_api_key)

# Step 4: Set correct permissions
!chmod 600 ~/.kaggle/kaggle.json

# Step 5: Download the UCF Crime dataset directly to Colab (Auto-extract)
!kaggle datasets download -d odins0n/ucf-crime-dataset --unzip

# Step 6: Verify that the dataset is downloaded
import os
print(os.listdir("/content"))

!pip install kaggle opencv-python tensorflow numpy

# create a kaggle directory
! mkdir ~/.kaggle
# copy the kaggle.json file to the kaggle directory
! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d odins0n/ucf-crime-dataset --unzip

import os
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# Load Pretrained ResNet-50 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(pretrained=True)
model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove classification layer
model.to(device).eval()

# Define image transformation
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Standard input size for ResNet-50
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Function to extract features from an image
def extract_features(image_path):
    image = Image.open(image_path).convert('RGB')
    image = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        return model(image).squeeze().cpu().numpy()

# Paths (Change these to your dataset paths)
data_path = "/content/Train/RoadAccidents"  # Folder containing only .png images
output_path = "/content/extracted_RoadAccidents"  # Folder to save extracted features
os.makedirs(output_path, exist_ok=True)

# Process dataset
features = []
frame_names = []

# Get a list of image files in the data path and filter for .png files
image_files = [f for f in os.listdir(data_path) if f.endswith('.png')]

# Iterate through the image files using enumerate to get index (i)
for i, frame in enumerate(sorted(image_files)):  # Ensure sorted order
    frame_path = os.path.join(data_path, frame)
    print(f"[{i+1}/{len(image_files)}] Extracting features from: {frame}")  # Debugging step

    feature_vector = extract_features(frame_path)
    features.append(feature_vector)
    frame_names.append(frame)  # Store frame names for reference

# Save extracted features as a .npy file
if features:
    np.save(os.path.join(output_path, "features.npy"), np.array(features))
    np.save(os.path.join(output_path, "frame_names.npy"), np.array(frame_names))  # Save frame names
    print(f"Saved features in {output_path}/features.npy")

print("Feature extraction completed.")

from google.colab import files

file_path = "/content/extracted_RoadAccidents/frame_names.npy"
files.download(file_path)

from google.colab import files

file_path = "/content/extracted_RoadAccidents/features.npy"
files.download(file_path)

import os
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# Load Pretrained ResNet-50 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(pretrained=True)
model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove classification layer
model.to(device).eval()

# Define image transformation
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Standard input size for ResNet-50
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Function to extract features from an image
def extract_features(image_path):
    image = Image.open(image_path).convert('RGB')
    image = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        return model(image).squeeze().cpu().numpy()

# Paths (Change these to your dataset paths)
data_path = "/content/Train/Arson"  # Folder containing only .png images
output_path = "/content/extracted_Arson"  # Folder to save extracted features
os.makedirs(output_path, exist_ok=True)

# Process dataset
features = []
frame_names = []

# Get a list of image files in the data path and filter for .png files
image_files = [f for f in os.listdir(data_path) if f.endswith('.png')]

# Iterate through the image files using enumerate to get index (i)
for i, frame in enumerate(sorted(image_files)):  # Ensure sorted order
    frame_path = os.path.join(data_path, frame)
    print(f"[{i+1}/{len(image_files)}] Extracting features from: {frame}")  # Debugging step

    feature_vector = extract_features(frame_path)
    features.append(feature_vector)
    frame_names.append(frame)  # Store frame names for reference

# Save extracted features as a .npy file
if features:
    np.save(os.path.join(output_path, "features.npy"), np.array(features))
    np.save(os.path.join(output_path, "frame_names.npy"), np.array(frame_names))  # Save frame names
    print(f"Saved features in {output_path}/features.npy")

print("Feature extraction completed.")

import os
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# Load Pretrained ResNet-50 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(pretrained=True)
model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove classification layer
model.to(device).eval()

# Define image transformation
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Standard input size for ResNet-50
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Function to extract features from an image
def extract_features(image_path):
    image = Image.open(image_path).convert('RGB')
    image = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        return model(image).squeeze().cpu().numpy()

# Paths (Change these to your dataset paths)
data_path = "/content/Train/Arrest"  # Folder containing only .png images
output_path = "/content/extracted_Arrest"  # Folder to save extracted features
os.makedirs(output_path, exist_ok=True)

# Process dataset
features = []
frame_names = []

# Get a list of image files in the data path and filter for .png files
image_files = [f for f in os.listdir(data_path) if f.endswith('.png')]

# Iterate through the image files using enumerate to get index (i)
for i, frame in enumerate(sorted(image_files)):  # Ensure sorted order
    frame_path = os.path.join(data_path, frame)
    print(f"[{i+1}/{len(image_files)}] Extracting features from: {frame}")  # Debugging step

    feature_vector = extract_features(frame_path)
    features.append(feature_vector)
    frame_names.append(frame)  # Store frame names for reference

# Save extracted features as a .npy file
if features:
    np.save(os.path.join(output_path, "features.npy"), np.array(features))
    np.save(os.path.join(output_path, "frame_names.npy"), np.array(frame_names))  # Save frame names
    print(f"Saved features in {output_path}/features.npy")

print("Feature extraction completed.")

import os
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# Load Pretrained ResNet-50 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(pretrained=True)
model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove classification layer
model.to(device).eval()

# Define image transformation
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Standard input size for ResNet-50
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Function to extract features from an image
def extract_features(image_path):
    image = Image.open(image_path).convert('RGB')
    image = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        return model(image).squeeze().cpu().numpy()

# Paths (Change these to your dataset paths)
data_path = "/content/Train/Arrest"  # Folder containing only .png images
output_path = "/content/extracted_Arrest"  # Folder to save extracted features
os.makedirs(output_path, exist_ok=True)

# Process dataset
features = []
frame_names = []

# Get a list of image files in the data path and filter for .png files
image_files = [f for f in os.listdir(data_path) if f.endswith('.png')]

# Iterate through the image files using enumerate to get index (i)
for i, frame in enumerate(sorted(image_files)):  # Ensure sorted order
    frame_path = os.path.join(data_path, frame)
    print(f"[{i+1}/{len(image_files)}] Extracting features from: {frame}")  # Debugging step

    feature_vector = extract_features(frame_path)
    features.append(feature_vector)
    frame_names.append(frame)  # Store frame names for reference

# Save extracted features as a .npy file
if features:
    np.save(os.path.join(output_path, "features.npy"), np.array(features))
    np.save(os.path.join(output_path, "frame_names.npy"), np.array(frame_names))  # Save frame names
    print(f"Saved features in {output_path}/features.npy")

print("Feature extraction completed.")

from google.colab import files

file_path = "/content/extracted_Arrest/frame_names.npy"
files.download(file_path)

from google.colab import files

file_path = "/content/extracted_Arrest/features.npy"
files.download(file_path)

import os
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image

# Load Pretrained ResNet-50 Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(pretrained=True)
model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove classification layer
model.to(device).eval()

# Define image transformation
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Standard input size for ResNet-50
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Function to extract features from an image
def extract_features(image_path):
    image = Image.open(image_path).convert('RGB')
    image = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        return model(image).squeeze().cpu().numpy()

# Paths (Change these to your dataset paths)
data_path = "/content/Train/NormalVideos"  # Folder containing only .png images
output_path = "/content/extracted_NormalVideos"  # Folder to save extracted features
os.makedirs(output_path, exist_ok=True)

# Process dataset
features = []
frame_names = []

# Get a list of image files in the data path and filter for .png files
image_files = [f for f in os.listdir(data_path) if f.endswith('.png')]

# Iterate through the image files using enumerate to get index (i)
for i, frame in enumerate(sorted(image_files)):  # Ensure sorted order
    frame_path = os.path.join(data_path, frame)
    print(f"[{i+1}/{len(image_files)}] Extracting features from: {frame}")  # Debugging step

    feature_vector = extract_features(frame_path)
    features.append(feature_vector)
    frame_names.append(frame)  # Store frame names for reference

# Save extracted features as a .npy file
if features:
    np.save(os.path.join(output_path, "features.npy"), np.array(features))
    np.save(os.path.join(output_path, "frame_names.npy"), np.array(frame_names))  # Save frame names
    print(f"Saved features in {output_path}/features.npy")

print("Feature extraction completed.")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import matplotlib.pyplot as plt


# Load Extracted Features
feature_path = "/content/features.npy"
labels_path = "/content/features.npy"  # Ensure labels are available

features = np.load(feature_path)  # Shape: (num_frames, 2048)
labels = np.load(labels_path)  # Shape: (num_frames,)

# Define Custom Dataset for LSTM
class AnomalyDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# Create Dataset & DataLoader
dataset = AnomalyDataset(features, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Define LSTM Model
class AnomalyLSTM(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=1):
        super(AnomalyLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = x.unsqueeze(0)  # Add batch dimension
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Use the last time step
        return self.sigmoid(out)

# Initialize Model
model = AnomalyLSTM().to(device)

# Loss & Optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training Loop
epochs = 20
train_losses, train_accuracies = [], []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for features, labels in dataloader:
        features, labels = features.to(device), labels.to(device).unsqueeze(1)

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        predicted = (outputs > 0.5).float()
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    train_losses.append(total_loss / len(dataloader))
    train_accuracies.append(correct / total)

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {train_losses[-1]:.4f}, Accuracy: {train_accuracies[-1]:.4f}")

# Save Model
torch.save(model.state_dict(), "/content/anomaly_lstm.pth")
print("Training completed and model saved.")

# Plot Training Performance
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(range(1, epochs+1), train_losses, label='Train Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy', color='green')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy')
plt.legend()

plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import matplotlib.pyplot as plt


# Load Extracted Features
feature_path = "/content/features.npy"
labels_path = "/content/features.npy"  # Ensure labels are available

features = np.load(feature_path)  # Shape: (num_frames, 2048)
labels = np.load(labels_path)  # Shape: (num_frames,)

# Define Custom Dataset for LSTM
class AnomalyDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# Create Dataset & DataLoader
dataset = AnomalyDataset(features, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Define LSTM Model
class AnomalyLSTM(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=1):
        super(AnomalyLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = x.unsqueeze(0)  # Add batch dimension
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Use the last time step
        return self.sigmoid(out)

# Define device (make sure it's defined in the current scope)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize Model
model = AnomalyLSTM().to(device)

# Loss & Optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training Loop
epochs = 20
train_losses, train_accuracies = [], []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for features, labels in dataloader:
        features, labels = features.to(device), labels.to(device).unsqueeze(1)

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        predicted = (outputs > 0.5).float()
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    train_losses.append(total_loss / len(dataloader))
    train_accuracies.append(correct / total)

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {train_losses[-1]:.4f}, Accuracy: {train_accuracies[-1]:.4f}")

# Save Model
torch.save(model.state_dict(), "/content/anomaly_lstm.pth")
print("Training completed and model saved.")

# Plot Training Performance
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(range(1, epochs+1), train_losses, label='Train Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy', color='green')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy')
plt.legend()

plt.show()

import numpy as np

# Load features to determine number of frames
feature_path = "/content/features.npy"
features = np.load(feature_path)  # Shape: (num_frames, 2048)
num_frames = features.shape[0]  # Get number of frames

# Assign a label (e.g., 1 for 'Arrest', change as needed)
label = 1
labels = np.full((num_frames,), label, dtype=np.int64)

# Save labels file
labels_path = "/content/labels.npy"
np.save(labels_path, labels)

print(f"Labels file saved at {labels_path}, Shape: {labels.shape}")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import matplotlib.pyplot as plt


# Load Extracted Features
feature_path = "/content/features.npy"
# Assuming you have a separate file for labels, adjust the path if needed
labels_path = "/content/frame_names.npy"  # Change to your labels file path

features = np.load("/content/features.npy")  # Shape: (num_frames, 2048)
labels = np.load("/content/labels.npy")  # Shape: (num_frames,)  # Assuming labels are 1D

# Define Custom Dataset for LSTM
class AnomalyDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# Create Dataset & DataLoader
dataset = AnomalyDataset(features, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Define LSTM Model
class AnomalyLSTM(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=1):
        super(AnomalyLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Reshape x to (batch_size, sequence_length, input_size)
        # Assuming sequence_length = 1 for individual frames
        x = x.view(x.size(0), 1, -1)
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Use the last time step
        return self.sigmoid(out)

# Define device (make sure it's defined in the current scope)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize Model
model = AnomalyLSTM().to(device)

# Loss & Optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training Loop
epochs = 20
train_losses, train_accuracies = [], []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for features, labels in dataloader:
        features, labels = features.to(device), labels.to(device).unsqueeze(1) # Add dimension for BCELoss

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)  # Calculate loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        predicted = (outputs > 0.5).float()
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    train_losses.append(total_loss / len(dataloader))
    train_accuracies.append(correct / total)

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {train_losses[-1]:.4f}, Accuracy: {train_accuracies[-1]:.4f}")

# Save Model
torch.save(model.state_dict(), "/content/anomaly_lstm.pth")
print("Training completed and model saved.")

# Plot Training Performance
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(range(1, epochs+1), train_losses, label='Train Loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy', color='green')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy')
plt.legend()

plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Load extracted features & labels
feature_path = "/content/features.npy"
labels_path = "/content/labels.npy"

features = np.load(feature_path)  # Shape: (num_samples, 2048)
labels = np.load(labels_path)  # Shape: (num_samples,)

# Split into 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels)

# Save train & test sets
np.save("/content/train_features.npy", X_train)
np.save("/content/train_labels.npy", y_train)
np.save("/content/test_features.npy", X_test)
np.save("/content/test_labels.npy", y_test)

print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Testing set: {X_test.shape}, {y_test.shape}")

class AnomalyDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

# Create train & test datasets
train_dataset = AnomalyDataset(X_train, y_train)
test_dataset = AnomalyDataset(X_test, y_test)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class AnomalyLSTM(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=1):
        super(AnomalyLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = x.view(x.size(0), 1, -1)  # Reshape to (batch_size, 1, input_size)
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Use last time step
        return self.sigmoid(out)

# Define device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize model
model = AnomalyLSTM().to(device)

# Binary Cross Entropy Loss (since labels are 0 or 1)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training Loop
epochs = 20
train_losses, train_accuracies = [], []

for epoch in range(epochs):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for features, labels in train_loader:
        features, labels = features.to(device), labels.to(device).unsqueeze(1) # Add dimension for BCELoss

        optimizer.zero_grad()
        outputs = model(features)
        loss = criterion(outputs, labels)  # Calculate loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        predicted = (outputs > 0.5).float()
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    train_losses.append(total_loss / len(train_loader))
    train_accuracies.append(correct / total)

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {train_losses[-1]:.4f}, Accuracy: {train_accuracies[-1]:.4f}")

# Save Model
torch.save(model.state_dict(), "/content/anomaly_lstm.pth")
print("Training completed and model saved.")

# Load Model for Testing
model.load_state_dict(torch.load("/content/anomaly_lstm.pth"))
model.eval()

test_loss = 0
correct = 0
total = 0

with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device).unsqueeze(1)
        outputs = model(features)
        loss = criterion(outputs, labels)
        test_loss += loss.item()

        predicted = (outputs > 0.5).float()
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

test_accuracy = correct / total
print(f"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.4f}")

import os
import numpy as np

# Path to the parent folder containing all 14 category folders
data_path = "/content/UCF-Crime-Features/"

# List all category names (subfolders)
categories = sorted(os.listdir(data_path))  # Sorting ensures label consistency
print("Detected Categories:", categories)

features = []
labels = []

# Assign labels (Normal = 0, Anomalies = 1-13)
for label, category in enumerate(categories):
    category_path = os.path.join(data_path, category)

    # Check if features.npy exists
    feature_file = os.path.join(category_path, "features.npy")
    if os.path.exists(feature_file):
        feature_vector = np.load(feature_file)  # Load feature vector

        # Store features and labels
        features.append(feature_vector)
        labels.append(np.full((feature_vector.shape[0],), label))  # Assign label to all frames

# Convert lists to NumPy arrays
features = np.vstack(features)  # Stack all feature vectors
labels = np.concatenate(labels)  # Combine labels

# Save combined dataset
np.save("/content/all_features.npy", features)
np.save("/content/all_labels.npy", labels)

print(f"Feature loading complete. Shape: {features.shape}, Labels: {labels.shape}")

